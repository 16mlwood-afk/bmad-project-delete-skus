name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: '3.13'

jobs:
  # Quality checks and testing
  quality:
    name: Code Quality & Tests
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y python3-dev

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r sku-cleanup-tool/requirements.txt
        pip install pytest pytest-cov coverage[toml] black isort mypy flake8

    - name: Run linting and formatting checks
      run: |
        # Stop on first error for faster feedback
        set -e

        # Code formatting check
        black --check --diff sku-cleanup-tool/

        # Import sorting check
        isort --check-only --diff sku-cleanup-tool/

        # Type checking
        mypy sku-cleanup-tool/ --ignore-missing-imports

        # Linting
        flake8 sku-cleanup-tool/ --max-line-length=100 --extend-ignore=E203,W503

    - name: Run tests with coverage
      run: |
        cd sku-cleanup-tool
        python -m pytest --cov=. --cov-report=xml --cov-report=html --cov-report=term-missing tests/

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./sku-cleanup-tool/coverage.xml
        flags: unittests
        name: codecov-umbrella

    - name: Generate test report
      if: always()
      run: |
        cd sku-cleanup-tool
        python -m pytest --cov=. --cov-report=html --html=test-report.html tests/
        echo "Test report generated: test-report.html"

    - name: Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results
        path: |
          sku-cleanup-tool/htmlcov/
          sku-cleanup-tool/coverage.xml
          sku-cleanup-tool/test-report.html

  # Performance testing
  performance:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: quality

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r sku-cleanup-tool/requirements.txt
        pip install pytest psutil

    - name: Run performance tests
      run: |
        cd sku-cleanup-tool
        python -m pytest tests/test_performance.py -v --tb=short

    - name: Generate performance report
      run: |
        cd sku-cleanup-tool
        python -c "
        import time
        from data_processor import DataProcessor
        from datetime import datetime, timedelta

        processor = DataProcessor()

        # Benchmark different dataset sizes
        sizes = [100, 1000, 5000, 10000]
        results = []

        for size in sizes:
            sku_data = []
            for i in range(size):
                age_days = 400 if i % 3 == 0 else 15
                created_date = (datetime.now() - timedelta(days=age_days)).strftime('%d/%m/%Y')
                sku_data.append({
                    'sku': f'PERF-{i:06d}',
                    'asin': f'B{i:011d}',
                    'created_date': created_date,
                    'fulfillment_channel': 'MERCHANT',
                    'quantity': 0,
                    'status': 'Active'
                })

            start_time = time.perf_counter()
            processed = processor.process_sku_data(sku_data)
            end_time = time.perf_counter()

            processing_time = end_time - start_time
            throughput = size / processing_time

            results.append(f'Size {size}: {throughput:.0f} SKUs/second, {processing_time:.2f}s total')

        print('Performance Benchmarks:')
        for result in results:
            print(f'  {result}')
        "

  # Security scanning
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: quality

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit[toml]

    - name: Run security vulnerability scan
      run: |
        cd sku-cleanup-tool
        safety check --json --output security-report.json || true

    - name: Run security linting
      run: |
        cd sku-cleanup-tool
        bandit -r . -f json -o bandit-report.json || true

    - name: Upload security reports
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          sku-cleanup-tool/security-report.json
          sku-cleanup-tool/bandit-report.json

  # Build and package
  build:
    name: Build & Package
    runs-on: ubuntu-latest
    needs: [quality, performance, security]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build setuptools wheel

    - name: Build package
      run: |
        python -m build --wheel --sdist

    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: python-package
        path: |
          dist/*.whl
          dist/*.tar.gz

  # Deploy to staging (on main branch)
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    environment: staging

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Add your staging deployment commands here
        # For example:
        # - Upload to PyPI test instance
        # - Deploy to staging server
        # - Run database migrations
        echo "✅ Deployment to staging completed"

  # Deploy to production (manual trigger)
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: build
    if: github.event_name == 'workflow_dispatch'

    environment: production

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        # Add your production deployment commands here
        # For example:
        # - Upload to PyPI
        # - Deploy to production server
        # - Update CDN
        # - Run smoke tests
        echo "✅ Deployment to production completed"

    - name: Post-deployment verification
      run: |
        echo "Running post-deployment checks..."
        # Add verification steps:
        # - Health checks
        # - Integration tests
        # - Performance validation
        echo "✅ Post-deployment verification completed"
